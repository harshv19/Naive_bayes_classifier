{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET-Devanagri\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as s\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_folder_names = os.listdir(\"./Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_images = []\n",
    "\n",
    "for each_folder in list_of_folder_names:\n",
    "    \n",
    "    base_path = os.path.join(\"./Train\",each_folder)\n",
    "    \n",
    "    list_of_images_in_folder = os.listdir(base_path)\n",
    "    \n",
    "    list_of_images.extend(map(lambda x: plt.imread(os.path.join(base_path,x)).reshape(1024,),list_of_images_in_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_up_images=np.array(list_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.DataFrame(data=stacked_up_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for i in range(0,46):\n",
    "    for j in range(0,1700):\n",
    "        labels.append(i)\n",
    "train_labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=pd.DataFrame(data=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=train_labels.rename(columns={0:'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([train_labels,raw_data],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# z normalisation of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing mean of every column and standard deviation of every column of the dataframe,except data['labels']\n",
    "raw_data_mean=[]\n",
    "raw_data_standard_dev=[]\n",
    "columns=list(raw_data.columns)\n",
    "for i in columns:\n",
    "    raw_data_mean.append(raw_data[i].mean())\n",
    "    raw_data_standard_dev.append(raw_data[i].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will subtract the mean of every column of the data from every value of the column and divide it with its standard deviation\n",
    "\n",
    "In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here,we will normalize our data to zero mean and one standard deviation\n",
    "for i in range(len(columns)):\n",
    "        if (raw_data_standard_dev[i]==0):\n",
    "            raw_data[i]=raw_data[i]-raw_data_mean[i]\n",
    "        else:\n",
    "            raw_data[i]=(raw_data[i]-raw_data_mean[i])/raw_data_standard_dev[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will calculate covariance matrix of our dataset\n",
    "covariance_matrix=raw_data.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will get the determinant to conclude whether the our covariance matrix is singular or not\n",
    "covariance_mat=np.linalg.det(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=np.array(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since in our dataset each class covariance matrix is singular now,we have to reduce the dimensionality of our dataset.we will first use PCA(principal component analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEPS TO REDUCE DIMENSION USING PCA\n",
    "\n",
    "1.COMPUTE EIGEN VECTOR AND EIGEN VALUES OF THE DATASET FROM COVARIANCE MATRIX.\n",
    "\n",
    "2.SET VARIANCE YOU WANT IN THE DATASET,RANGE(95%-99%)\n",
    "\n",
    "3.CREATE A SET OF EIGEN VECTORS WHOSE EIGEN VALUES SUM IS EQUAL TO THE PRESET VARIANCE\n",
    "\n",
    "4.TAKE A PROJECTION OF THE LOWER DIMENSION DATASET BY TAKING  THE DOT PRODUCT OF THE DATASET WITH THE SELECTED EIGEN VECTOR MATRIX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1. TO COMPUTE EIGEN VALUES AND EIGEN VECTOR WE WILL USE SVD(Singular value decomposition)\n",
    "eigen_vector,eigen_value,eigen_vector_transpose=np.linalg.svd(covariance_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the eigen vector and eigen value in the descending order\n",
    "#for sorting we are using quick sort,since quick sort will sort the array in ascending order we will reverse the solution\n",
    "#since we are using singular value decomposition there is no need to use this step,use when you are using different technique for getting eigen value and eigen vector\n",
    "sorted_eigen_vector=np.sort(eigen_vector,axis=0)\n",
    "sorted_eigen_value=np.sort(eigen_value)\n",
    "sorted_eigen_vector=sorted_eigen_vector[::-1]\n",
    "sorted_eigen_value=sorted_eigen_value[::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2. WE WILL HAVE VARIANCE OF 98%\n",
    "#STEP 3. WE WILL COMPUTE THE INDICES OF ALL EIGEN VECTOR BY WHICH OUR VARIANCE IS EQUAL TO 98%\n",
    "principal_components=[]\n",
    "total=np.sum(eigen_value)\n",
    "current_sum=0\n",
    "variance_to_preserve=0.98\n",
    "for i in range(1024):\n",
    "    current_sum+=eigen_value[i]\n",
    "    if((current_sum/total)>variance_to_preserve):\n",
    "        break\n",
    "    principal_components.append(eigen_vector[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are those eigen vectors which contribute to the 99% of the variance of the dataset.\n",
    "important_eigen_vectors=np.array(principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(important_eigen_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the length function used on the calculated eigen vectors from the pca tells us that 297 eigen_vectors out of 1024 are having 98 percent variance of the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now project the lower dimensions of this dataset via multiplying the dataset with eigen vectors we computed above having 99% of the variance.\n",
    "new_data=np.matmul(raw_data,important_eigen_vectors.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the covariance of the new dataset for each classes.\n",
    "cov_mats_reduced_of_each_class=[]\n",
    "for i in range(46):\n",
    "    cov_mats_reduced_of_each_class.append(np.cov(new_data[i*1700:(i+1)*1700,:],rowvar=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determinant of the new dataset for each class\n",
    "det=[]\n",
    "for i in range(46):\n",
    "    det.append(np.linalg.det(cov_mats_reduced_of_each_class[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower dimension dataset.\n",
    "new_data=pd.DataFrame(data=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.662810895808441e-152\n",
      "5.5896908648348955e-192\n",
      "1.7836929593040674e-161\n",
      "2.3790903174931527e-136\n",
      "2.849775189487269e-148\n",
      "1.486286998157952e-181\n",
      "3.064896876384309e-176\n",
      "1.3889392679070817e-126\n",
      "3.562947546188586e-128\n",
      "2.462335681254776e-122\n",
      "3.879679915492212e-111\n",
      "3.199407100078146e-138\n",
      "5.8862876443065354e-182\n",
      "2.9563777857393568e-145\n",
      "2.628362037937387e-108\n",
      "2.268547340179328e-115\n",
      "6.182244038371368e-121\n",
      "8.863660209661719e-130\n",
      "1.0550352764986484e-157\n",
      "2.738994823706418e-147\n",
      "2.447462039207096e-134\n",
      "1.7733658377221643e-78\n",
      "1.4892686907976684e-129\n",
      "5.13926698700883e-150\n",
      "4.2649340522743084e-96\n",
      "1.2473873107899989e-114\n",
      "5.201731288699862e-109\n",
      "1.5784890056011085e-143\n",
      "1.5987737159818723e-109\n",
      "1.1318937884043946e-174\n",
      "4.620638108333877e-123\n",
      "4.299961040664245e-110\n",
      "1.4941163972585244e-154\n",
      "1.8025557623136834e-94\n",
      "2.2131856089244031e-150\n",
      "1.4904396169046249e-89\n",
      "0.0\n",
      "2.9670249831695477e-261\n",
      "5.29354903192228e-193\n",
      "1.538307801546597e-157\n",
      "2.400706179259682e-206\n",
      "6.931997625425209e-172\n",
      "1.3793204983847247e-144\n",
      "1.6990132891048343e-194\n",
      "9.030355615607336e-242\n",
      "1.3539720916397797e-209\n"
     ]
    }
   ],
   "source": [
    "for i in range(46):\n",
    "    print(det[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since our new dataset's covariance is still very zero,hence we will use regularized discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the covariance of each class in the pca transform dataset.\n",
    "cov_m=[]\n",
    "for i in range(46):\n",
    "    cov_m.append(new_data.iloc[i*1700:(i+1)*1700].cov())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating pooled covariance matrix.\n",
    "pool=0\n",
    "for i in cov_m:\n",
    "    pool=pool+1699*i\n",
    "pool=pool/(78200-46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1615001816987016"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating sigma \n",
    "sigma=np.trace(pool)/pool.shape[0]\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rda(cov_m,pool,sigma):\n",
    "    #using the maximum alpha and gamma value obtained above to get the optimum rda covariance matrix for our dataset. \n",
    "    alpha=np.random.uniform(0,1)\n",
    "    gamma=np.random.uniform(0,1)\n",
    "    final_cov_rda=[]\n",
    "    for i in cov_m:\n",
    "          final_cov_rda.append((alpha*i)+(1-alpha)*pool)\n",
    "    #using the optimum rda covariance matrix to get the modified rda covariance matrix\n",
    "    final_mod_rda=[]\n",
    "    for i in final_cov_rda:\n",
    "        final_mod_rda.append(((1-gamma)*i)+(gamma*sigma*np.identity(cov_m[0].shape[0])))\n",
    "    return final_mod_rda,alpha,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(data):\n",
    "    #calculating mean of each class of the pca transformed data ie.(new data)\n",
    "    mean_rda=[]\n",
    "    for i in range(46):\n",
    "        mean_rda.append(new_data.iloc[i*1700:(i+1)*1700].mean())\n",
    "    return mean_rda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_probab(data,rda):\n",
    "    #calculating posterior probablity using probablity distribution function of the normal distribution,as we assume our data is coming from gauusian distribution.\n",
    "    #this will give us likelihood probablity but we maximize the likelihood probability by using -log hence getting posterior probability.\n",
    "    mean_rda1=mean(new_data)\n",
    "    posterior_train_probabilities=[]\n",
    "    for i in range(46):\n",
    "        posterior_train_probabilities.append(-np.log(s.multivariate_normal.pdf(data,mean_rda1[i],rda[i])))\n",
    "    return posterior_train_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_cal(posterior_train_probabilities):\n",
    "    #there are 46 arrays in the likelihood probabilities each representing a class\n",
    "    #each array has 78200 probabilities that signifies the probabilities of each image\n",
    "    ans_train=[]\n",
    "    for i in range(78200):\n",
    "        m=[]\n",
    "        for j in range(46):\n",
    "            m.append((posterior_train_probabilities[j][i],j))\n",
    "        ans_train.append(min(m))\n",
    "    return ans_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best():\n",
    "    accuracy=[]\n",
    "    alphalist=[]\n",
    "    gammalist=[]\n",
    "    for i in range(20):\n",
    "        final_mod_rda,alpha,gamma=rda(cov_m,pool,sigma)\n",
    "        posterior_train_probabilities=posterior_probab(new_data,final_mod_rda)\n",
    "        ans_train=predictions_cal(posterior_train_probabilities)\n",
    "        predicted_labels=[]\n",
    "        for j in range(78200):\n",
    "            predicted_labels.append(ans_train[j][1])\n",
    "        predicted_labels=np.array(predicted_labels).reshape(78200,1)\n",
    "        boolean_mask=(predicted_labels == train_labels)\n",
    "        accuracy.append((np.count_nonzero(boolean_mask)/78200)*100)\n",
    "        alphalist.append(alpha)\n",
    "        gammalist.append(gamma)\n",
    "    ind=np.argmax(accuracy)\n",
    "    best_accuracy=accuracy[ind]\n",
    "    best_alpha=alphalist[ind]\n",
    "    best_gamma=gammalist[ind]\n",
    "    \n",
    "    return best_accuracy,best_alpha,best_gamma\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A-Team\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "accuracy,alpha,gamma=best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.5306905370844"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_rda(cov_m,pool,sigma,alpha,gamma):\n",
    "    best_rda=[]\n",
    "    for i in cov_m:\n",
    "          best_rda.append((alpha*i)+(1-alpha)*pool)\n",
    "    #using the optimum rda covariance matrix to get the modified rda covariance matrix\n",
    "    best_rda_dash=[]\n",
    "    for i in best_rda:\n",
    "        best_rda_dash.append(((1-gamma)*i)+(gamma*sigma*np.identity(cov_m[0].shape[0])))\n",
    "    return best_rda_dash\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rda_final=final_rda(cov_m,pool,sigma,alpha,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_testing_folder_names = os.listdir(\"./Test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_testing_images = []\n",
    "\n",
    "for each_folder in list_of_testing_folder_names:\n",
    "    \n",
    "    base_path = os.path.join(\"./Test/\",each_folder)\n",
    "    \n",
    "    list_of_testing_images_in_folder = os.listdir(base_path)\n",
    "    \n",
    "    list_of_testing_images.extend(map(lambda x: plt.imread(os.path.join(base_path,x)).reshape(1024,),list_of_testing_images_in_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_stacked_up_images=np.array(list_of_testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_raw_data=pd.DataFrame(data=testing_stacked_up_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13798"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_raw_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(columns)):\n",
    "        if (raw_data_standard_dev[i]==0):\n",
    "            testing_raw_data[i]=testing_raw_data[i]-raw_data_mean[i]\n",
    "        else:\n",
    "            testing_raw_data[i]=(testing_raw_data[i]-raw_data_mean[i])/raw_data_standard_dev[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will use the above pca computed eigen vectors to project the lower dimension of the testing dataset.\n",
    "new_testing_data=testing_raw_data.dot(important_eigen_vectors.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the mean of each class of the new_testing_data.\n",
    "pca_transform_data_mean=[]\n",
    "for i in range(46):\n",
    "    pca_transform_data_mean.append(new_testing_data.iloc[i*300:(i+1)*300].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating posterior probability of the testing dataset\n",
    "testing_posterior_probability=[]\n",
    "for i in range(46):\n",
    "    testing_posterior_probability.append(-np.log(s.multivariate_normal.pdf(new_testing_data,pca_transform_data_mean[i],rda_final[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 46 arrays in the likelihood probabilities each representing a class\n",
    "#each array has 78200 probabilities that signifies the probabilities of each image\n",
    "ans_test=[]\n",
    "for i in range(new_testing_data.shape[0]):\n",
    "    n=[]\n",
    "    for j in range(46):\n",
    "        n.append((testing_posterior_probability[j][i],j))\n",
    "    ans_test.append(min(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels=[]\n",
    "for i in range(new_testing_data.shape[0]):\n",
    "    predicted_test_labels.append(ans_test[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels=np.array(predicted_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels=predicted_test_labels.reshape(13798,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "listi_test=[]\n",
    "for i in range(0,46):\n",
    "    for j in range(0,300):\n",
    "        listi_test.append(i)\n",
    "test_labels=np.array(listi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=test_labels[0:13798]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13798,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_mask_test=np.count_nonzero(predicted_test_labels == test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12488"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean_mask_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification accuracy\n",
    "AC=TN+TP/TN+FP+FN+TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.50587041600232\n"
     ]
    }
   ],
   "source": [
    "#Accuracy-proportion or percentage of correctly predicted labels over all predictions\n",
    "accuracy=(boolean_mask_test/new_testing_data.shape[0])*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
